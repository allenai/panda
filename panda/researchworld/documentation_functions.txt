
USEFUL PYTHON FUNCTIONS
=======================

1. RESEARCH TOOLS
-----------------

def create_dataset(prompt:str, item_col:str='question', temperature:int=0):
Purpose:
    Generate a dataset of items (e.g., questions) using prompt, and place in a new DataFrame under item_col
Args:
    prompt (str): The prompt querying GPT for dataset items. The prompt MUST mention the number of items to return.
        Note that items don't have be questions: they can also scenarios, situations, etc.
        It is good practice to include an example item (e.g., question) in the prompt.
    item_col (str) (optional, default 'question'): The column to put the items under
    temperature (int): The temperature to use during LLM generation (default 0)
Returns:
    DataFrame: A new dataframe containing the items
Example:
    dataset = create_dataset("Generate 30 test questions that test simple two-digit addition, e.g., '23 + 43 = ?'", item_col='question')
    print(dataset)
->     question
    0  35 + 47 = ?
    1  58 + 16 = ?
    2  72 + 29 = ?
    ...
Example:
    print(create_dataset("Generate 30 moral dilemmas, e.g., 'John was faced with stealing food to save his family.'", item_col='dilemma'))
->     dilemma
    0  A doctor must choose between saving a young child with a rare disease or using the limited medicine to save five elderly patients.
    1  An engineer knows about a flaw in a bridge's design that could lead to collapse but revealing it would cause panic and cost their job.
...

----------------------------------------

def answer_questions(dataset:pd.DataFrame, prompt_template:str, answer_col:str, model:str):
Purpose:
    For every row in the dataset, query the model with the instantiated prompt_template, and put answers under a new column called answer_col.
Args:
    dataset (DataFrame): the dataset, containing items (e.g., questions) under a particular column (e.g., 'question')
    prompt_template (str): The template prompt to query model with. The template reference the item column
    answer_col (str): The DataFrame column to place the answers in
    model (str): The model to query. For now, valid answers are 'gpt4', 'gpt-4.1', 'gpt-4.1-nano', 'llama', 'mistral', 'claude', 'o1-mini', 'o3-mini', 'o4-mini'
Returns:
    DataFrame: The dataset DataFrame updated with the answers. (Note: the dataset Dataframe is destructively updated)
Example:
    dataset = pd.DataFrame([{'question':'What is 1 + 1?'}, {'question':'What is 2 + 2?'}])
    answer_questions(dataset, "Answer this question: {question}", answer_col='answer', model='llama')
    print(dataset)
 ->          question answer
    0  What is 1 + 1?      2
    1  What is 2 + 2?      4

----------------------------------------

def score_answers(dataset:pd.DataFrame, prompt_template:str, score_col='score', score_range=10, model:str=agent_config.PANDA_LLM):    
Purpose:
    Use model (LM-as-judge) to score a set of answers in dataset. Scores are added to the dataset DataFrame.
    The function queries the model for every row in the data frame with the instantiated prompt_template, 
    and collects the scoring information as an instantiated json_template.
Args:
    dataset (DataFrame): The dataset
    prompt_template (str): Describes how to score individual answers in each row of the dataset
    score_col (str): The column to put the scores in (default 'score').
    score_range (int): The range of scores, e.g., if scores are 0 to 10, then the score_range is 10. This is used for score normalization.
    model (str) (optional): The model to do the scoring. Valid models are 'gpt4', 'o1-mini' and 'o3-mini'.
Returns:
    DataFrame: The dataset DataFrame updated with the scores in score_col, and also a justification in column {score_col}_justification
Example:
    dataset = pd.DataFrame([{"question":"What is the sum of 34 and 21?","answer":"The sum of 34 and 21 is 55."},
                            {"question":"Add 58 and 36 together.","answer":"Add 58 and 36: 94."}])
    score_answers(dataset, "Score the answer to the following question between 0 (completely wrong) and 10 (completely correct):
Question: {question}
Answer: {answer}", score_col='score', score_range=10)
    print(dataset)
                        question                       answer    score                                                   score_justification
0  What is the sum of 34 and 21?  The sum of 34 and 21 is 55. 1.000000  The answer is completely correct. The sum of 34 and 21 is indeed 55.
1        Add 58 and 36 together.           Add 58 and 36: 94. 1.000000                  The answer is completely correct. 58 + 36 equals 94.

----------------------------------------

def ideate_categories(dataset:DataFrame, item_col:str='question', score_col:str='score', highlow:str='high', n=None):
Purpose:
    Given a dataset of items, and a metric (e.g., score), speculate on categories that cover items with high values for that metric, but not items with low values.
    Essentially, each category should partition the dataset according to metric.
Args:
    dataset (DataFrame): A DataFrame of objects (e.g., questions)
    item_col (str): The column of dataset listing the objects
    score_col (str): The dataset metric to use for partitioning objects
    highlow (str): If 'high', then categories should cover objects with high metric values. If 'low', they should cover objects with low metric values.
    n (int) (optional): The number of categories to ideate
Returns:
    categories (DataFrame): A new DataFrame containing the ideated categories, with the following fields (one per category):
        title: The name of the ideated category
        description: A description of the category
        score: the average data_metric just for dataset examples in this category
        adj_score: An adjusted score, to account for categories with few examples in. The adj_score is the one to use when reasoning about the category.
        n_covered: the number of dataset examples in this category
        f_covered: the fraction of dataset examples in this category
        signal: The OVERALL QUALITY, or discriminative-ness, of this category, defined as the difference between the adj_score and overall average score in the dataset.
Notes: 
    - The categories are returned sorted, those with the highest (lowest) signal first, except the first row (iloc[0], loc[0]) representing the special category "everything".
      Thus you can select the best (highest signal) category via categories.iloc[1].
    - If there are more than ~30 items in the dataset, scoring is just done with a random sample from that dataset
    - The row with index 0 contains the special category "Everything", covering the entire dataset (n_covered = the full random sample size used for scoring)

Example:
    dataset = pd.DataFrame([{"question":"1+1?","answer":2,"score":1.0},
                            {"question":"20+20?","answer":43,"score":0.0},
                            {"question":"23+33?","answer":123,"score":0.0},
                            {"question":"2+2?","answer":4,"score":1.0}])
    difficult_categories = ideate_categories(dataset, item_col='question', score_col='score', highlow='low', n=3)          # difficult categories have LOW score
    print(difficult_categories)   # categories of "difficult" (low-scoring) questions
                                     title                                                            description    score  n_covered  f_covered  adj_score    signal
    0                           everything                                                     The entire dataset 0.500000          4   1.000000   0.500000 -0.000000
    1          Addition of multiples of 20  Questions involving the addition of numbers that are multiples of 20. 0.000000          1   0.250000   0.454545  0.045455
    2       Addition resulting in above 40                            Questions where the sum is greater than 40. 0.000000          1   0.250000   0.454545  0.045455
    3  Addition with both addends above 20          Questions where both numbers being added are greater than 20. 0.000000          1   0.250000   0.454545  0.045455
Example:
    easy_categories = ideate_categories(dataset, item_col='question', score_col='score', highlow='high', n=3)
    print(easy_categories)
->                 title                                                      description    score  n_covered  f_covered  adj_score   signal
    0              everything                                               The entire dataset 0.500000          4   1.000000   0.500000 0.000000
    1  Single Digit Additions  Addition problems where all numbers involved are single digits. 1.000000          2   0.500000   0.583333 0.083333
    easiest_category = easy_categories.iloc[1]

----------------------------------------

def examples_in_category(dataset:pd.DataFrame, category_row:pd.Series, score_col='score', highlow='high', n=9999):
Purpose: 
    Find examples (rows in dataset) of a given category (a row from a categories DataFrame)
Args:
    dataset (DataFrame): A dataset of items (QA pairs, etc.)
    category_row (Series): A row from a categories DataFrame, representing a particular ideated category
    score_col (str): The column in dataset containing the score (metric) by which dataset items are being evaluated
    highlow (str): one of 'high' or 'low', indicating if the category aims to select items with unusually high or low score
    n (int) (optional): the number of dataset examples to return. (default = return all examples in category)
Returns:
    DataFrame: A subset of n rows in dataset, containing dataset items in the target category. The ones with
               highest (lowest) score_col are selected first.
Example:
    dataset = pd.DataFrame([{"question":"1+1?","answer":2,"score":1.0},
                            {"question":"20+20?","answer":43,"score":0.0},
                            {"question":"23+33?","answer":123,"score":0.0},
                            {"question":"2+2?","answer":4,"score":1.0}])
    easy_categories = ideate_categories(dataset, item_col='question', score_col='score', highlow='high', n=3)
    print(easy_categories)
->                 title                                                      description    score  n_covered  f_covered  adj_score   signal
    0              everything                                               The entire dataset 0.500000          4   1.000000   0.500000 0.000000
    1  Single Digit Additions  Addition problems where all numbers involved are single digits. 1.000000          2   0.500000   0.583333 0.083333
    easiest_category = easy_categories.iloc[1]
    print(examples_in_category(dataset, easiest_category, score_col='score', highlow='high', n=1))
->    question  answer    score 
    0     1+1?       2 1.000000 

3. STATISTICS
-------------

def spearman_strength(spearman_corr:float):
Purpose:
    Convert a Spearman rank correlation coefficient to a qualitative word, according to this conversion table:
	 .00-.19 very weak
	 .20-.39 weak
	 .40-.59 moderate
	 .60-.79 strong
	 .80-1.0 very strong
Args:
    spearman_corr (float): The Spearman correlation coefficient (range -1 to 1)
Returns:
    One of the five words above, interpreting the number
Example:
    print(spearman_strength(0.54))
-> moderate

----------------------------------------

def pearson_strength(pearson_corr:float):
Purpose:
    Convert a Pearson correlation coefficient to a qualitative word, according to this conversion table:
	 .00-.30 very weak
	 .30-.50 weak
	 .50-.70 moderate
	 .70-.90 strong
	 .90-1.0 very strong
Args:
    spearman_corr (float): The Spearman correlation coefficient (range -1 to 1)
Returns:
    One of the five words above, interpreting the number
Example:
    print(spearman_strength(0.54))
-> moderate

4. BASIC CALLS TO A LLM
-----------------------

def call_llm(prompt:str, model:str, temperature:int=0):
Purpose:
    Basic access to an LLM.
Args:
    prompt (str): The question/instruction to give to the LLM.
    model (str): One of 'gpt4', 'gpt-4.1', 'gpt-4.1-nano', 'llama', 'mistral', 'claude', 'o1-mini', 'o3-mini', 'o4-mini'
    temperature (int): The temperature to use during LLM generation (default 0)
Returns:
    response (str): The LLM response.
Example:
    call_llm("Generate a new research idea about large language models.")
->  Title: Investigating the Impact of Multimodal Inputs on Large Language ....

----------------------------------------

def llm_list(prompt:str, model:str):
Purpose:
    Get a list of string answers from an LLM in a single call. This function queries the LLM with the prompt, extracts a list of answers, and returns them as a list.
Args:
    prompt (str): The prompt to be sent to GPT. The prompt should specify the number of items to return. 
                  It is good practice to include an example of the item to return in the prompt.
    model (str): The LLM to call. Currently must be one of 'gpt4' or 'o1-mini'.
Returns:
    list: A list of items (strings)
Example:
    print(llm_list("Generate 3 test questions for a children's quiz that test simple two-digit addition, e.g., 'What is 23 + 34?'"))
 -> ['What is 34 + 21?', 'How much is 47 + 15?', 'Add 58 and 36 together.']
Example:
    print(llm_list("Tell me some countries."))
 -> ['China', 'India', 'United States', 'Indonesia', 'Pakistan']
